{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AEDZG6DycaQp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from xgboost import XGBRegressor\n",
        "#from xgboost import plot_importance\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "import statsmodels.api as sm\n",
        "import json\n",
        "from rh2q_2d import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import transforms\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "iopath='/content/drive/MyDrive/THEQC/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GINsNTdP0t9L",
        "outputId": "e2f3dc70-ce06-4289-e669-26918bb57b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x6Fk_6B8c-wn"
      },
      "outputs": [],
      "source": [
        "def t2the(p,t,q):\n",
        "    Lv=2.5e6\n",
        "    Cp=1004.\n",
        "    Rd=287.\n",
        "    th=(t+273.15)*((1000./p)**(Rd/Cp))\n",
        "    the=th*np.exp((Lv*(q/1000.))/(Cp*(t+273.15)))\n",
        "\n",
        "    return th, the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nLQZbfuynBNk"
      },
      "outputs": [],
      "source": [
        "def data_split(nday):\n",
        "    # Split the dataset: 80% for training and 20% for test\n",
        "    fsize=np.arange(nday)\n",
        "    ftrain0, ftest0=train_test_split(fsize, random_state=777, train_size=0.8)\n",
        "    ftrain=sorted(ftrain0)\n",
        "    ftest=sorted(ftest0)\n",
        "\n",
        "    return ftrain, ftest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZGYHR918-Ytj"
      },
      "outputs": [],
      "source": [
        "def preprocess(data,label):\n",
        "  if label=='st':\n",
        "    p1=data[0,:,:]/1100.\n",
        "    t1=(data[1,:,:]-(-50.))/100.\n",
        "    rh1=data[2,:,:]/100.\n",
        "    u1=(data[3,:,:]-(-50.))/100.\n",
        "    v1=(data[4,:,:]-(-50.))/100.\n",
        "    rad1=data[5,:,:]/1400.\n",
        "    q1=data[6,:,:]/data[7,:,:]\n",
        "    th1=(data[8,:,:]-300.)/100.\n",
        "    the1=(data[9,:,:]-300.)/100.\n",
        "    st_data=np.array([p1,t1,rh1,u1,v1,rad1,q1,th1,the1],dtype='float')\n",
        "    return st_data\n",
        "  elif label=='vs':\n",
        "    p1=data[0,:,:]/1100.\n",
        "    t1=(data[1,:,:]-(-50.))/100.\n",
        "    rh1=data[2,:,:]/100.\n",
        "    q1=data[3,:,:]/data[4,:,:]\n",
        "    th1=(data[5,:,:]-300.)/100.\n",
        "    the1=(data[6,:,:]-300.)/100.\n",
        "    vs_data=np.array([p1,t1,rh1,q1,th1,the1],dtype='float')\n",
        "    return vs_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7BqF36Aoj67q"
      },
      "outputs": [],
      "source": [
        "def load_data(stpath,vspath):\n",
        "    #ST\n",
        "    stinf=pd.read_csv(stpath)\n",
        "    nlev=701\n",
        "    nrow=stinf.shape[0]\n",
        "    nday=int(nrow/nlev)\n",
        "    date=stinf['date'].astype('int').values.reshape(nday,nlev)\n",
        "    p_st=stinf['P'].values.reshape(nday,nlev)\n",
        "    t_st=stinf['T'].values.reshape(nday,nlev)\n",
        "    rh_st=stinf['RH'].values.reshape(nday,nlev)\n",
        "    u_st=stinf['U'].values.reshape(nday,nlev)\n",
        "    v_st=stinf['V'].values.reshape(nday,nlev)\n",
        "    rad_st=stinf['rad'].values.reshape(nday,nlev)\n",
        "    t_diff_st=stinf['T_diff'].values.reshape(nday,nlev)\n",
        "    rh_diff_st=stinf['RH_diff'].values.reshape(nday,nlev)\n",
        "    # Calculate specific humidity/theta, thetae\n",
        "    q_st, q0_st=rh_to_q(p_st,t_st,rh_st)\n",
        "    q_st=stinf['q'].values.reshape(nday,nlev)\n",
        "    th_st, the_st=t2the(p_st,t_st,q_st)\n",
        "\n",
        "    # VS\n",
        "    vsinf=pd.read_csv(vspath)\n",
        "    p_vs=vsinf['P'].values.reshape(nday,nlev)\n",
        "    t_vs=vsinf['T'].values.reshape(nday,nlev)\n",
        "    rh_vs=vsinf['RH'].values.reshape(nday,nlev)\n",
        "    # Calculate specific humidity/theta, thetae\n",
        "    q_vs, q0_vs=rh_to_q(p_vs,t_vs,rh_vs)\n",
        "    q_vs=vsinf['q'].values.reshape(nday,nlev)\n",
        "    th_vs, the_vs=t2the(p_vs,t_vs,q_vs)\n",
        "\n",
        "    # Calculate q/th difference\n",
        "    q_diff=q_vs-q_st\n",
        "    th_diff=th_vs-th_st\n",
        "    the_diff=the_vs-the_st\n",
        "\n",
        "    st_data=np.array([p_st,t_st,rh_st,u_st,v_st,rad_st,q_st,q0_st,th_st,the_st],dtype='float')\n",
        "    vs_data=np.array([p_vs,t_vs,rh_vs,q_vs,q0_vs,th_vs,the_vs],dtype='float')\n",
        "\n",
        "    st_input=preprocess(st_data,'st')\n",
        "    vs_input=preprocess(vs_data,'vs')\n",
        "\n",
        "    id_train, id_test=data_split(nday)\n",
        "    st_train=st_input[:,id_train,:]\n",
        "    st_test=st_input[:,id_test,:]\n",
        "\n",
        "    np.save(iopath+'st_train.npy',st_train)\n",
        "    np.save(iopath+'st_test.npy',st_test)\n",
        "    vs_train=vs_input[:,id_train,:]\n",
        "    vs_test=vs_input[:,id_test,:]\n",
        "    np.save(iopath+'vs_train.npy',vs_train)\n",
        "    np.save(iopath+'vs_test.npy',vs_test)\n",
        "    the_diff_train=the_diff[id_train,:]\n",
        "    the_diff_test=the_diff[id_test,:]\n",
        "    date_train=date[id_train,:]\n",
        "    date_test=date[id_test,:]\n",
        "\n",
        "    return st_train, st_test, vs_train, vs_test, the_diff_train, the_diff_test,date_train,date_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YNNkYsHzHvDc"
      },
      "outputs": [],
      "source": [
        "def nd21d(st3d,vs3d,the2d):\n",
        "    st2d=st3d.reshape(st3d.shape[0],st3d.shape[1]*st3d.shape[2])\n",
        "    vs2d=vs3d.reshape(vs3d.shape[0],vs3d.shape[1]*vs3d.shape[2])\n",
        "    the1d_0=the2d.reshape(the2d.shape[0]*the2d.shape[1])\n",
        "    #st_data=np.array([p1,t1,rh1,u1,v1,rad1,q1,th1,the1],dtype='float')\n",
        "    #vs_data=np.array([p1,t1,rh1,q1,th1,the1],dtype='float')\n",
        "    # Remove missing values\n",
        "    # Filtering: no missing values in ST's T, radiation and q\n",
        "    mask=np.zeros(st2d.shape[1],dtype='int')\n",
        "    for i in range(len(mask)):\n",
        "        if np.isnan(st2d[1,i])==False and np.isnan(st2d[2,i])==False and np.isnan(st2d[5,i])==False and np.isnan(the1d_0[i])==False:\n",
        "           mask[i]+=1\n",
        "    st1d=st2d[:,mask==1]\n",
        "    st1d=pd.DataFrame(st1d)\n",
        "    st1d.index=['P','T','RH','U','V','rad','q','th','the']\n",
        "    st1d.T.to_csv(iopath+'STIO.csv',index=False)\n",
        "    vs1d=vs2d[:,mask==1]\n",
        "    vs1d=pd.DataFrame(vs1d)\n",
        "    vs1d.index=['P','T','RH','q','th','the']\n",
        "    vs1d.T.to_csv(iopath+'VSIO.csv',index=False)\n",
        "    the1d=the1d_0[mask==1]\n",
        "    return st1d.T, vs1d.T, the1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "umnLSu93ROLC"
      },
      "outputs": [],
      "source": [
        "def evaluation(model,x,y,model_name,var):\n",
        "    print(\"===== RFE feature selection =====\")\n",
        "    #from sklearn.model_selection import StratifiedKFold\n",
        "    from sklearn.feature_selection import RFECV\n",
        "    # Create the RFE object and compute a cross-validation score.\n",
        "    rfecv=RFECV(estimator=model, step=1,cv=10)\n",
        "    rfecv.fit(x,y)\n",
        "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
        "\n",
        "    ranking = rfecv.ranking_\n",
        "    print(list(ranking))\n",
        "\n",
        "    # Plot number of features VS. cross-validation scores\n",
        "    #plt.figure()\n",
        "    #plt.xlabel(\"Number of features selected\")\n",
        "    #plt.ylabel(\"Cross validation score\")\n",
        "    #plt.plot(range(1, len(t_rfecv.grid_scores_) + 1), t_rfecv.grid_scores_)\n",
        "    #plt.savefig('glm_all_t_rfecv10.png')\n",
        "    #plt.savefig('glm_noUV_t_rfecv10.png')\n",
        "    #plt.close()\n",
        "\n",
        "    #x_sel=list(x1[:,rfecv.support_])\n",
        "    print(list(rfecv.support_))\n",
        "\n",
        "    # Comprehensive evaluation\n",
        "    glm_sm=sm.OLS(y,x)\n",
        "    summary=glm_sm.fit(maxiter=30).summary()\n",
        "    print(summary)\n",
        "\n",
        "    #with open(model_name+'_summary.txt','w') as f:\n",
        "    #     for lines in summary:\n",
        "    #         lines.write('%s' %line)\n",
        "\n",
        "    # P-value and F-score\n",
        "    f_score, p_val=f_regression(x,y)\n",
        "    print('F-value: '+str(f_score))\n",
        "    print('p-value: '+str(p_val))\n",
        "\n",
        "    f_sort=f_score\n",
        "    print(var.shape)\n",
        "    v_sort=np.array(var)\n",
        "    for j in range(len(f_score)):\n",
        "        for i in range(len(f_score)-1):\n",
        "            if f_score[i] > f_score[i+1]:\n",
        "               tmp=f_sort[i]\n",
        "               f_sort[i]=f_sort[i+1]\n",
        "               f_sort[i+1]=tmp\n",
        "\n",
        "               v_tmp=v_sort[i]\n",
        "               v_sort[i]=v_sort[i+1]\n",
        "               v_sort[i+1]=v_tmp\n",
        "\n",
        "    fscore_df=pd.DataFrame(f_sort)\n",
        "    fscore_df.index=v_sort\n",
        "\n",
        "    df=pd.DataFrame()\n",
        "    df.insert(0,'Ranking',ranking,True)\n",
        "    df.insert(1,'Support',rfecv.support_,True)\n",
        "    df.insert(2,'F-score',f_score,True)\n",
        "    df.insert(3,'p-value',p_val,True)\n",
        "    df.index=var\n",
        "    #df.to_csv(model_name+'_score.csv')\n",
        "\n",
        "    # Plotting\n",
        "    fsize=18\n",
        "    y_pos=np.arange(len(f_score))\n",
        "    plt.figure(figsize=(12,7))\n",
        "    plt.barh(y_pos,f_sort,align='center',height=0.2)\n",
        "    plt.grid()\n",
        "    plt.yticks(y_pos,v_sort,fontsize=fsize)\n",
        "    plt.xticks(fontsize=fsize)\n",
        "    plt.xlabel('F-value',fontsize=fsize)\n",
        "    plt.ylabel('Features',fontsize=fsize)\n",
        "    #plt.title(model_name+' model feature importance',fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(iopath+model_name+'_score.png')\n",
        "    plt.close()\n",
        "\n",
        "def glm(x,y):\n",
        "    print('===== Training Linear Regression Model  =====')\n",
        "    model=LinearRegression().fit(x,y)\n",
        "    print(model.score(x,y))\n",
        "    coef={'GLM_thetae_coef':model.coef_}\n",
        "    dft=pd.DataFrame(coef)\n",
        "    dft.index=['P','T','RH','U','V','rad','q','th','the']\n",
        "    dft.to_csv(iopath+'glm_thetae_coef.csv', header=None)\n",
        "\n",
        "    # Save the model as a pickle object\n",
        "    with open(iopath+'glm_thetae.pkl','wb') as f:\n",
        "         pickle.dump(model,f)\n",
        "    evaluation(model,x,y,'glm_thetae',x.columns)\n",
        "    del model\n",
        "    return y\n",
        "\n",
        "def predict(x):\n",
        "    with open(iopath+'glm_thetae.pkl','rb') as f:\n",
        "         model=pickle.load(f)\n",
        "    y_pred=model.predict(x)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WiVz-VBRqQlw",
        "outputId": "5251f20a-4db9-4983-d4e3-e60ef8886b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                [50, 1, 64]             640\n",
            "            Linear-2                [50, 1, 32]           2,080\n",
            "            Linear-3                [50, 1, 16]             528\n",
            "            Linear-4                 [50, 1, 1]              17\n",
            "================================================================\n",
            "Total params: 3,265\n",
            "Trainable params: 3,265\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 0.06\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class FCNN(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super(FCNN,self).__init__()\n",
        "    self.fc1=nn.Linear(input_dim,64)\n",
        "    self.fc2=nn.Linear(64,32)\n",
        "    self.fc3=nn.Linear(32,16)\n",
        "    self.output = nn.Linear(16, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=F.relu(self.fc2(x))\n",
        "    x=F.relu(self.fc3(x))\n",
        "    x = self.output(x)\n",
        "\n",
        "    return x\n",
        "summary(FCNN(9),input_size=(1,9),batch_size=50,device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6R6bOdQGuSrJ"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y_pred, y):\n",
        "    mse = torch.nn.functional.mse_loss(y_pred, y)\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JZgd5EkkhT0e"
      },
      "outputs": [],
      "source": [
        "def scoring(y_pred,st_the,vs_the):\n",
        "    y=(st_the*100.+300.)+y_pred\n",
        "    vs_the=vs_the*100.+300.\n",
        "    rmse=np.sqrt(mean_squared_error(vs_the,y))\n",
        "    corr=np.corrcoef(vs_the,y)[0,1]\n",
        "    print('Correlation: '+str(corr))\n",
        "    print('RMSE: '+str(rmse))\n",
        "    return corr,rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2NEjtppvdBge"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Load ST and VS iodata\n",
        "    st_train, st_test, vs_train, vs_test, the_diff_train, the_diff_test, date_train, date_test=load_data('/content/drive/MyDrive/MLQC/st_io_1hPa.csv'\n",
        "          ,'/content/drive/MyDrive/MLQC/vs_io_1hPa.csv')\n",
        "\n",
        "    print(\"Training dataset:\")\n",
        "    st_train_1d, vs_train_1d, the_diff_train_1d=nd21d(st_train, vs_train, the_diff_train)\n",
        "    print(st_train_1d.shape)\n",
        "    print(vs_train_1d.shape)\n",
        "    print(the_diff_train_1d.shape)\n",
        "    print(\"Testing dataset:\")\n",
        "    st_test_1d, vs_test_1d, the_diff_test_1d=nd21d(st_test,vs_test,the_diff_test)\n",
        "    print(st_test_1d.shape)\n",
        "    print(vs_test_1d.shape)\n",
        "    print(the_diff_test_1d.shape)\n",
        "\n",
        "    # Train GLM\n",
        "    #y_pred_train=glm(st_train_1d,the_diff_train_1d)\n",
        "    #scoring(y_pred_train,st_train_1d['the'],vs_train_1d['the'])\n",
        "\n",
        "    # Test GLM\n",
        "    #y_pred=predict(st_test_1d)\n",
        "    #scoring(y_pred,st_test_1d['the'],vs_test_1d['the'])\n",
        "\n",
        "    # Training NN model\n",
        "    print(\"===== Training Fully-connected NN T-model =====\")\n",
        "\n",
        "    # Use GPU\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    epochs = 100\n",
        "    batch_size = 50\n",
        "    nvar = 9\n",
        "    epo_print=1\n",
        "\n",
        "    #  Convert to torch object\n",
        "    input_data=torch.from_numpy(st_train_1d.values).float()\n",
        "    target_data=torch.from_numpy(the_diff_train_1d).float()\n",
        "\n",
        "    #  Dataloader\n",
        "    dataset = TensorDataset(input_data, target_data)\n",
        "    train_loader = DataLoader(dataset = dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    # Load Fully-connected NN model to gpu\n",
        "    fcnn_t = FCNN(input_dim=nvar).to(device)\n",
        "    fcnn_t.train()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(fcnn_t.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training\n",
        "    e_loss=[]\n",
        "    min_loss = 99999999999\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        tot_loss=0.\n",
        "        for batch_inputs, batch_targets in train_loader:\n",
        "            batch_inputs=batch_inputs.to(device)\n",
        "            batch_targets=batch_targets.to(device)\n",
        "            fcnn_t.zero_grad()\n",
        "            outputs = fcnn_t(batch_inputs)\n",
        "            #print(outputs[:,0],batch_targets)\n",
        "\n",
        "            loss = loss_fn(outputs[:,0], batch_targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tot_loss+=loss.item()\n",
        "\n",
        "        tot_loss=tot_loss/len(train_loader.dataset)\n",
        "\n",
        "\n",
        "        if epoch % epo_print ==0:\n",
        "            e_loss.append(tot_loss)\n",
        "            updated = False\n",
        "            if min_loss > tot_loss:\n",
        "                updated = True\n",
        "                min_loss = tot_loss\n",
        "                torch.save(fcnn_t, iopath+'FCNN_THE.pkl')\n",
        "            print (\n",
        "            '[{:>5d}/{:>5d}]'.format(epoch+1, epochs),\n",
        "            'Loss:{:>.8e}, '.format(tot_loss),\n",
        "            'updated = {:>6s}, min loss={:>.8e}'.format(str(updated),min_loss)\n",
        "            )\n",
        "\n",
        "    del fcnn_t\n",
        "\n",
        "    # Testing NN model\n",
        "    print(\"===== Testing Fully-connected NN Theta_e-model =====\")\n",
        "    fcnn_t=torch.load(iopath+'FCNN_THE.pkl').to(device)\n",
        "    fcnn_t.eval()\n",
        "    input_data=input_data.to(device)\n",
        "    output=fcnn_t(input_data)\n",
        "    output=output.detach().cpu().numpy()\n",
        "    print(output.shape)\n",
        "    scoring(output[:,0],st_train_1d['the'],vs_train_1d['the'])\n",
        "\n",
        "    # Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "J88CmSpjJCqF",
        "outputId": "10b87d8e-2175-434e-f675-c2a2e5886fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset:\n",
            "(471791, 9)\n",
            "(471791, 6)\n",
            "(471791,)\n",
            "Testing dataset:\n",
            "(116492, 9)\n",
            "(116492, 6)\n",
            "(116492,)\n",
            "===== Training Fully-connected NN T-model =====\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3c93b4b78d8b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-7fca758789b5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtot_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "   main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qnBuTUkrTPO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}